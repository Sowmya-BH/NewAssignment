import os
import streamlit as st
from groq import Groq
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize Groq client
client = Groq(api_key=os.environ["GROQ_API_KEY"])


def initialize_session_state():
    """Initializes session state for chat history."""
    if "history" not in st.session_state:
        st.session_state.history = []


def display_ui():
    """Displays the main UI components: title, sidebar, and chat history.

    Returns:
        str: The selected LLM model from the sidebar.
    """
    # Title
    st.title("üí¨ Hi, I'm Nexus.ai!")
    st.title("How can I help you Today??")
    st.markdown("<hr style='border: 1px solid #ccc; opacity: 0.5;'>", unsafe_allow_html=True)

    # Sidebar
    st.sidebar.markdown(
        """
        ---
        <h3 style="font-size: 2.0em; font-weight: bold;">ü§ñ Nexus.ai</h3>
        """,
        unsafe_allow_html=True,
    )

    st.sidebar.markdown(
        """
        ---
        ### üñê Banana Facts:
        * I use multiple LLMs.
        * You can select different models here.
        """
    )

    model = st.sidebar.selectbox(
        'Choose your LLM',
        ['Llama3-8b-8192', 'Llama3-70b-8192', 'Mixtral-8x7b-32768', 'Gemma-7b-It']
    )

    # Chat History in Sidebar
    st.sidebar.markdown(
        """
        ---
        <h5 style="font-size: 1.0em; font-weight: bold;">Chat History</h5>
        """,
        unsafe_allow_html=True,
    )

    for i, entry in enumerate(st.session_state.history):
        if st.sidebar.button(f'ü§ñ {entry["query"][:20]}...', key=f"hist_{i}", help=entry["query"]):
            display_response(entry["response"])

    return model


def get_user_input():
    """Gets user input from the text input box.

    Returns:
        str: The text entered by the user.
    """
    user_input = st.text_input("Enter your query: ", "")
    return user_input


def query_llm(user_input: str, model: str) -> str:
    """Queries the Groq LLM with the user input and selected model.

    Args:
        user_input: The query string from the user.
        model: The name of the LLM model to use.

    Returns:
        str: The response generated by the LLM.
    """
    chat_completion = client.chat.completions.create(
        messages=[{"role": "user", "content": user_input}],
        model=model,
    )
    response = chat_completion.choices[0].message.content
    return response


def display_response(response: str):
    """Displays the LLM response in the main area.

    Args:
        response: The text response from the LLM.
    """
    st.markdown(f'<div class="response-box">{response}</div>', unsafe_allow_html=True)


def update_chat_history(query: str, response: str):
    """Stores the user query and LLM response in the session history.

    Args:
        query: The user's input query.
        response: The LLM's response to the query.
    """
    st.session_state.history.append({"query": query, "response": response})


def main():
    """Main function to run the Streamlit application."""
    initialize_session_state()
    model = display_ui()
    user_input = get_user_input()

    if st.button("Submit"):
        if user_input:
            response = query_llm(user_input, model)
            update_chat_history(user_input, response)
            display_response(response)


if __name__ == "__main__":
    main()
